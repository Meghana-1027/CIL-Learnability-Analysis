{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkflTI07xaKD",
        "outputId": "b46ee151-6b70-48a1-f5fd-c288595a571f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_dim, feature_dim):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, feature_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class TaskHead(nn.Module):\n",
        "    def __init__(self, feature_dim, output_dim):\n",
        "        super(TaskHead, self).__init__()\n",
        "        self.fc = nn.Linear(feature_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class ROWModel(nn.Module):\n",
        "    def __init__(self, input_dim, feature_dim, num_tasks, task_classes):\n",
        "        super(ROWModel, self).__init__()\n",
        "        self.feature_extractor = FeatureExtractor(input_dim, feature_dim)\n",
        "        # Create WP and OOD heads for each task\n",
        "        self.wp_heads = nn.ModuleList([TaskHead(feature_dim, task_classes[i]) for i in range(num_tasks)])\n",
        "        self.ood_heads = nn.ModuleList([TaskHead(feature_dim, task_classes[i] + 1) for i in range(num_tasks)])\n",
        "\n",
        "    def forward(self, x, task_id, head_type='wp'):\n",
        "        features = self.feature_extractor(x)\n",
        "        if head_type == 'wp':\n",
        "            return self.wp_heads[task_id](features)\n",
        "        elif head_type == 'ood':\n",
        "            return self.ood_heads[task_id](features)\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def evaluate_accuracy(model, data_loader, task_id, head_type='wp', device='cpu'):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs, task_id, head_type=head_type)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    accuracy = 100 * correct / total if total > 0 else 0\n",
        "    print(f\"Accuracy for Task {task_id} ({head_type}): {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Example parameters (adjust as needed)\n",
        "input_dim = 784  # Example for a flattened 28x28 input\n",
        "feature_dim = 256\n",
        "num_tasks = 5\n",
        "task_classes = [2, 2, 2, 2, 2]  # Adjust based on number of classes per task\n",
        "\n",
        "# Initialize the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ROWModel(input_dim, feature_dim, num_tasks, task_classes).to(device)\n",
        "\n",
        "# Example data loader for testing accuracy (replace with actual data loader)\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Dummy data for testing\n",
        "x_test = torch.randn(100, input_dim)  # 100 samples, each of size `input_dim`\n",
        "y_test = torch.randint(0, 2, (100,))  # 100 labels, binary classification for example\n",
        "\n",
        "# Create a DataLoader\n",
        "test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=32)\n",
        "\n",
        "# Check accuracy for task 0 on WP head\n",
        "evaluate_accuracy(model, test_loader, task_id=0, head_type='wp', device=device)\n",
        "\n"
      ],
      "metadata": {
        "id": "aE2vogz_242O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d157e1ff-2f13-4b03-8db9-3a84fbf112c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for Task 0 (wp): 52.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52.0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Example replay buffer structure (for storing replay data)\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = []\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def add(self, data):\n",
        "        self.buffer.append(data)\n",
        "        if len(self.buffer) > self.max_size:\n",
        "            self.buffer.pop(0)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        if len(self.buffer) == 0:\n",
        "            return None, None\n",
        "        sampled_data = random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
        "        data, labels = zip(*sampled_data)\n",
        "        return torch.stack(data), torch.tensor(labels)\n",
        "\n",
        "# Training function for Step 1\n",
        "def train_feature_and_ood(model, task_id, optimizer, current_task_data, replay_buffer, epochs=5, batch_size=32):\n",
        "    \"\"\"\n",
        "    Trains the feature extractor and OOD head using current task data (IND) and replay buffer (OOD).\n",
        "\n",
        "    Args:\n",
        "    - model: ROWModel instance.\n",
        "    - task_id: Current task identifier.\n",
        "    - optimizer: Optimizer for training.\n",
        "    - current_task_data: DataLoader containing current task's data.\n",
        "    - replay_buffer: ReplayBuffer instance with data from previous tasks.\n",
        "    - epochs: Number of epochs to train.\n",
        "    - batch_size: Batch size for training.\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        for ind_data, ind_labels in current_task_data:\n",
        "            ind_data, ind_labels = ind_data.to(device), ind_labels.to(device)\n",
        "\n",
        "            # Sample OOD data from replay buffer\n",
        "            ood_data, ood_labels = replay_buffer.sample(batch_size)\n",
        "            if ood_data is None or ood_labels is None:\n",
        "                # Skip training on OOD if replay buffer is empty\n",
        "                print(\"Replay buffer is empty, skipping OOD training for this batch.\")\n",
        "                continue\n",
        "\n",
        "            ood_data, ood_labels = ood_data.to(device), ood_labels.to(device)\n",
        "\n",
        "            # Combine IND and OOD data\n",
        "            combined_data = torch.cat([ind_data, ood_data])\n",
        "            combined_labels = torch.cat([ind_labels, ood_labels])\n",
        "\n",
        "            # Set OOD labels to a unique class index in the OOD head\n",
        "            combined_labels[len(ind_labels):] = model.ood_heads[task_id].fc.out_features - 1\n",
        "\n",
        "            # Train on OOD head\n",
        "            optimizer.zero_grad()\n",
        "            ood_outputs = model(combined_data, task_id, head_type='ood')\n",
        "\n",
        "            # Compute cross-entropy loss for both IND and OOD data\n",
        "            loss = F.cross_entropy(ood_outputs, combined_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss for epoch-level summary\n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            # Print batch-level loss\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_count}], Batch Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        avg_epoch_loss = epoch_loss / batch_count if batch_count > 0 else 0\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] completed. Average Loss: {avg_epoch_loss:.4f}\\n\")\n",
        "\n",
        "# Example usage\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assume `model` is already defined and moved to the device\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Dummy current task data and replay buffer\n",
        "input_dim = 784  # Example input dimension, such as a flattened 28x28 image\n",
        "x_current = torch.randn(100, input_dim)  # 100 samples\n",
        "y_current = torch.randint(0, 2, (100,))  # Binary labels for example\n",
        "current_task_data = DataLoader(TensorDataset(x_current, y_current), batch_size=batch_size)\n",
        "\n",
        "# Initialize replay buffer and add some dummy data\n",
        "replay_buffer = ReplayBuffer(max_size=200)\n",
        "for i in range(50):\n",
        "    x_replay = torch.randn(input_dim)\n",
        "    y_replay = torch.tensor(2)  # Example OOD label\n",
        "    replay_buffer.add((x_replay, y_replay))\n",
        "\n",
        "# Train model on the task\n",
        "train_feature_and_ood(model, task_id=0, optimizer=optimizer, current_task_data=current_task_data, replay_buffer=replay_buffer, epochs=5, batch_size=batch_size)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "svdZSHKHx990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cb3d266-cfc2-4040-f4c7-be180577479e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Batch [1], Batch Loss: 1.0778\n",
            "Epoch [1/5], Batch [2], Batch Loss: 1.0185\n",
            "Epoch [1/5], Batch [3], Batch Loss: 0.9639\n",
            "Epoch [1/5], Batch [4], Batch Loss: 0.5875\n",
            "Epoch [1/5] completed. Average Loss: 0.9119\n",
            "\n",
            "Epoch [2/5], Batch [1], Batch Loss: 0.7373\n",
            "Epoch [2/5], Batch [2], Batch Loss: 0.6982\n",
            "Epoch [2/5], Batch [3], Batch Loss: 0.7139\n",
            "Epoch [2/5], Batch [4], Batch Loss: 0.2400\n",
            "Epoch [2/5] completed. Average Loss: 0.5973\n",
            "\n",
            "Epoch [3/5], Batch [1], Batch Loss: 0.4897\n",
            "Epoch [3/5], Batch [2], Batch Loss: 0.4232\n",
            "Epoch [3/5], Batch [3], Batch Loss: 0.4251\n",
            "Epoch [3/5], Batch [4], Batch Loss: 0.0931\n",
            "Epoch [3/5] completed. Average Loss: 0.3578\n",
            "\n",
            "Epoch [4/5], Batch [1], Batch Loss: 0.2345\n",
            "Epoch [4/5], Batch [2], Batch Loss: 0.1825\n",
            "Epoch [4/5], Batch [3], Batch Loss: 0.1864\n",
            "Epoch [4/5], Batch [4], Batch Loss: 0.0294\n",
            "Epoch [4/5] completed. Average Loss: 0.1582\n",
            "\n",
            "Epoch [5/5], Batch [1], Batch Loss: 0.0873\n",
            "Epoch [5/5], Batch [2], Batch Loss: 0.0632\n",
            "Epoch [5/5], Batch [3], Batch Loss: 0.0657\n",
            "Epoch [5/5], Batch [4], Batch Loss: 0.0088\n",
            "Epoch [5/5] completed. Average Loss: 0.0562\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def fine_tune_wp_head(model, task_id, optimizer, current_task_data, device, epochs=5):\n",
        "    \"\"\"\n",
        "    Fine-tunes the WP head for the current task using only the in-distribution data.\n",
        "\n",
        "    Args:\n",
        "    - model: ROWModel instance.\n",
        "    - task_id: Current task identifier.\n",
        "    - optimizer: Optimizer for training.\n",
        "    - current_task_data: DataLoader containing current task's data.\n",
        "    - device: Torch device (CPU or GPU).\n",
        "    - epochs: Number of epochs to train.\n",
        "    \"\"\"\n",
        "    # Freeze the feature extractor to keep it unchanged during WP head fine-tuning\n",
        "    for param in model.feature_extractor.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Check if DataLoader has data\n",
        "    if len(current_task_data) == 0:\n",
        "        print(\"DataLoader is empty.\")\n",
        "        return\n",
        "\n",
        "    # Begin fine-tuning process\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for batch_idx, (ind_data, ind_labels) in enumerate(current_task_data):\n",
        "            ind_data, ind_labels = ind_data.to(device), ind_labels.to(device)\n",
        "\n",
        "            # Forward pass for WP head\n",
        "            optimizer.zero_grad()\n",
        "            wp_outputs = model(ind_data, task_id, head_type='wp')\n",
        "\n",
        "            # Compute cross-entropy loss for WP head on IND data\n",
        "            loss = F.cross_entropy(wp_outputs, ind_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss for epoch\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy for the current batch\n",
        "            _, predicted = torch.max(wp_outputs, 1)\n",
        "            correct += (predicted == ind_labels).sum().item()\n",
        "            total += ind_labels.size(0)\n",
        "\n",
        "            # Print batch-level debug information\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}], Batch Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Calculate and print average loss and accuracy for the epoch\n",
        "        avg_loss = epoch_loss / len(current_task_data)\n",
        "        accuracy = 100 * correct / total if total > 0 else 0\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] completed. WP Head Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Unfreeze the feature extractor for further training if needed\n",
        "    for param in model.feature_extractor.parameters():\n",
        "        param.requires_grad = True\n"
      ],
      "metadata": {
        "id": "0tkwgpAAyMCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_ood_heads(model, optimizer, replay_buffer, current_task_id, epochs=3, batch_size=32):\n",
        "    \"\"\"\n",
        "    Fine-tunes the OOD heads of all previous tasks using the replay buffer.\n",
        "\n",
        "    Args:\n",
        "    - model: ROWModel instance.\n",
        "    - optimizer: Optimizer for training.\n",
        "    - replay_buffer: ReplayBuffer instance with data from previous tasks.\n",
        "    - current_task_id: The current task identifier.\n",
        "    - epochs: Number of epochs to train.\n",
        "    - batch_size: Batch size for training.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for task_id in range(current_task_id):  # Only fine-tune up to the previous task heads\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            for batch_data in replay_buffer.sample(batch_size):\n",
        "                ood_data, ood_labels = batch_data  # OOD data and labels from replay buffer\n",
        "                ood_data, ood_labels = ood_data.to(device), ood_labels.to(device)\n",
        "\n",
        "                # Set OOD labels to the special OOD class index\n",
        "                ood_labels[:] = model.ood_heads[task_id].out_features - 1\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                ood_outputs = model(ood_data, task_id, head_type='ood')\n",
        "\n",
        "                # Calculate OOD loss\n",
        "                loss = F.cross_entropy(ood_outputs, ood_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / len(replay_buffer.buffer)\n",
        "            print(f\"Task {task_id}, Epoch [{epoch+1}/{epochs}], OOD Head Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Example usage for fine-tuning OOD heads after learning each new task\n",
        "# fine_tune_ood_heads(model, optimizer, replay_buffer, current_task_id=current_task_id)\n"
      ],
      "metadata": {
        "id": "dhcqE-fiyODP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def inference(model, x, num_tasks):\n",
        "    \"\"\"\n",
        "    Makes a prediction using both WP and OOD heads for all tasks.\n",
        "\n",
        "    Args:\n",
        "    - model: ROWModel instance.\n",
        "    - x: Input tensor for inference.\n",
        "    - num_tasks: Total number of tasks learned so far.\n",
        "\n",
        "    Returns:\n",
        "    - Predicted label for the input instance x.\n",
        "    \"\"\"\n",
        "    x = x.to(device)\n",
        "    model.eval()\n",
        "    max_prob = 0\n",
        "    predicted_label = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for task_id in range(num_tasks):\n",
        "            # Calculate WP probability for the task\n",
        "            wp_output = model(x, task_id, head_type='wp')\n",
        "            wp_prob = F.softmax(wp_output, dim=1)\n",
        "\n",
        "            # Calculate OOD probability for the task\n",
        "            ood_output = model(x, task_id, head_type='ood')\n",
        "            ood_prob = F.softmax(ood_output, dim=1)\n",
        "\n",
        "            # Multiply WP and OOD probabilities as per the theory in the paper\n",
        "            combined_prob = wp_prob * ood_prob[:, :-1]  # Exclude the OOD class from wp_prob\n",
        "            max_task_prob, task_pred = combined_prob.max(dim=1)\n",
        "\n",
        "            # Update if this task has a higher probability than previous tasks\n",
        "            if max_task_prob.item() > max_prob:\n",
        "                max_prob = max_task_prob.item()\n",
        "                predicted_label = task_pred.item()\n",
        "\n",
        "    return predicted_label\n"
      ],
      "metadata": {
        "id": "hhO6qaxtyZyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_data_loaders, num_tasks):\n",
        "    \"\"\"\n",
        "    Evaluates the model on test data for each task.\n",
        "\n",
        "    Args:\n",
        "    - model: ROWModel instance.\n",
        "    - test_data_loaders: A list of DataLoaders, one for each task.\n",
        "    - num_tasks: Number of tasks the model has learned.\n",
        "\n",
        "    Returns:\n",
        "    - Accuracy for each task.\n",
        "    \"\"\"\n",
        "    accuracies = []\n",
        "\n",
        "    for task_id in range(num_tasks):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for x, y in test_data_loaders[task_id]:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            predicted_label = inference(model, x, num_tasks)\n",
        "            correct += (predicted_label == y.item()).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        accuracies.append(accuracy)\n",
        "        print(f\"Accuracy for Task {task_id}: {accuracy:.2f}%\")\n",
        "\n",
        "    return accuracies\n"
      ],
      "metadata": {
        "id": "A7u5SlCBym6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "def get_task_dataloaders(num_tasks, batch_size=32):\n",
        "    \"\"\"\n",
        "    Splits CIFAR-100 into multiple tasks, each with a distinct set of classes.\n",
        "\n",
        "    Args:\n",
        "    - num_tasks: Number of tasks to split the dataset into.\n",
        "    - batch_size: Batch size for data loaders.\n",
        "\n",
        "    Returns:\n",
        "    - A list of DataLoaders, one for each task.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-100 dataset\n",
        "    dataset = CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Split dataset into tasks\n",
        "    num_classes_per_task = len(dataset.classes) // num_tasks\n",
        "    task_dataloaders = []\n",
        "    test_dataloaders = []\n",
        "\n",
        "    for task_id in range(num_tasks):\n",
        "        # Get class indices for this task\n",
        "        task_classes = list(range(task_id * num_classes_per_task, (task_id + 1) * num_classes_per_task))\n",
        "\n",
        "        # Filter dataset by these classes\n",
        "        train_indices = [i for i, label in enumerate(dataset.targets) if label in task_classes]\n",
        "        test_indices = [i for i, label in enumerate(test_dataset.targets) if label in task_classes]\n",
        "\n",
        "        train_subset = Subset(dataset, train_indices)\n",
        "        test_subset = Subset(test_dataset, test_indices)\n",
        "\n",
        "        # Create DataLoader for this task\n",
        "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        task_dataloaders.append(train_loader)\n",
        "        test_dataloaders.append(test_loader)\n",
        "\n",
        "    return task_dataloaders, test_dataloaders\n",
        "\n",
        "# Example usage\n",
        "num_tasks = 5  # Define the number of tasks\n",
        "batch_size = 32\n",
        "train_loaders, test_loaders = get_task_dataloaders(num_tasks, batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC2UHz47y0yV",
        "outputId": "af9af5a1-f78a-4068-e447-4a7bab725c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:02<00:00, 69.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    }
  ]
}