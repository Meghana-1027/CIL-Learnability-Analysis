{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG24ygeVqnyQ",
        "outputId": "65622609-8dd5-4637-978e-f18947ed6780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Loaded 10 tasks with 10 classes each.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Load CIFAR-100 and split it into tasks for incremental learning\n",
        "def get_task_dataloaders(num_tasks=10, batch_size=32):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "    dataset = CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    num_classes_per_task = len(dataset.classes) // num_tasks\n",
        "    train_loaders, test_loaders = [], []\n",
        "\n",
        "    for task_id in range(num_tasks):\n",
        "        task_classes = list(range(task_id * num_classes_per_task, (task_id + 1) * num_classes_per_task))\n",
        "        train_indices = [i for i, label in enumerate(dataset.targets) if label in task_classes]\n",
        "        test_indices = [i for i, label in enumerate(test_dataset.targets) if label in task_classes]\n",
        "\n",
        "        train_subset = Subset(dataset, train_indices)\n",
        "        test_subset = Subset(test_dataset, test_indices)\n",
        "\n",
        "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        train_loaders.append(train_loader)\n",
        "        test_loaders.append(test_loader)\n",
        "\n",
        "    print(f\"Loaded {num_tasks} tasks with {num_classes_per_task} classes each.\")\n",
        "    return train_loaders, test_loaders\n",
        "\n",
        "# Define replay buffer to hold samples from previous tasks\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size=2000, num_tasks=10):\n",
        "        self.max_size_per_task = max_size // num_tasks\n",
        "        self.buffers = {task_id: deque(maxlen=self.max_size_per_task) for task_id in range(num_tasks)}\n",
        "\n",
        "    def add(self, task_id, data):\n",
        "        self.buffers[task_id].append(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        samples_per_task = batch_size // len(self.buffers)\n",
        "        batch_data = []\n",
        "\n",
        "        for task_id, buffer in self.buffers.items():\n",
        "            if len(buffer) > 0:\n",
        "                batch_data.extend(random.sample(list(buffer), min(len(buffer), samples_per_task)))\n",
        "\n",
        "        if len(batch_data) > 0:\n",
        "            batch_inputs, batch_labels = zip(*batch_data)\n",
        "            return torch.stack(batch_inputs), torch.tensor(batch_labels)\n",
        "        else:\n",
        "            return None, None\n",
        "\n",
        "# Initialize data loaders and replay buffer\n",
        "num_tasks = 10\n",
        "batch_size = 32\n",
        "train_loaders, test_loaders = get_task_dataloaders(num_tasks=num_tasks, batch_size=batch_size)\n",
        "replay_buffer = ReplayBuffer(max_size=2000, num_tasks=num_tasks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_dim=3*32*32, feature_dim=256):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, feature_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten input\n",
        "        return self.fc(x)\n",
        "\n",
        "class TaskHead(nn.Module):\n",
        "    def __init__(self, feature_dim, output_dim):\n",
        "        super(TaskHead, self).__init__()\n",
        "        self.fc = nn.Linear(feature_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class ROWModel(nn.Module):\n",
        "    def __init__(self, input_dim=3*32*32, feature_dim=256, num_classes_per_task=10, num_tasks=10):\n",
        "        super(ROWModel, self).__init__()\n",
        "        self.feature_extractor = FeatureExtractor(input_dim, feature_dim)\n",
        "        # WP heads for classification\n",
        "        self.wp_heads = nn.ModuleList([TaskHead(feature_dim, num_classes_per_task) for _ in range(num_tasks)])\n",
        "        # OOD heads for OOD detection\n",
        "        self.ood_heads = nn.ModuleList([TaskHead(feature_dim, num_classes_per_task + 1) for _ in range(num_tasks)])\n",
        "\n",
        "    def forward(self, x, task_id, head_type='wp'):\n",
        "        features = self.feature_extractor(x)\n",
        "        if head_type == 'wp':\n",
        "            return self.wp_heads[task_id](features)\n",
        "        elif head_type == 'ood':\n",
        "            return self.ood_heads[task_id](features)\n",
        "\n"
      ],
      "metadata": {
        "id": "VEwYL53irxzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_feature_and_ood_head(model, task_id, optimizer, train_loader, replay_buffer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct_ind, correct_ood = 0, 0\n",
        "        total_ind, total_ood = 0, 0\n",
        "\n",
        "        for ind_data, ind_labels in train_loader:\n",
        "            ind_data, ind_labels = ind_data.to(device), ind_labels.to(device)\n",
        "\n",
        "            # Sample OOD data from replay buffer\n",
        "            ood_data, ood_labels = replay_buffer.sample(len(ind_data))\n",
        "            if ood_data is None or ood_labels is None:\n",
        "                # If replay buffer is empty, skip OOD training for this batch\n",
        "                combined_data, combined_labels = ind_data, ind_labels\n",
        "            else:\n",
        "                ood_data, ood_labels = ood_data.to(device), ood_labels.to(device)\n",
        "                ood_labels[:] = model.ood_heads[task_id].fc.out_features - 1  # Set OOD labels to a unique class\n",
        "                combined_data = torch.cat([ind_data, ood_data])\n",
        "                combined_labels = torch.cat([ind_labels, ood_labels])\n",
        "\n",
        "            # Forward pass and compute loss\n",
        "            optimizer.zero_grad()\n",
        "            ood_outputs = model(combined_data, task_id, head_type='ood')\n",
        "            loss = F.cross_entropy(ood_outputs, combined_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy for IND and OOD\n",
        "            _, predicted = torch.max(ood_outputs, 1)\n",
        "            correct_ind += (predicted[:len(ind_labels)] == ind_labels).sum().item()\n",
        "            if ood_data is not None:\n",
        "                correct_ood += (predicted[len(ind_labels):] == ood_labels).sum().item()\n",
        "                total_ood += len(ood_labels)\n",
        "            total_ind += len(ind_labels)\n",
        "\n",
        "        ind_accuracy = 100 * correct_ind / total_ind if total_ind > 0 else 0\n",
        "        ood_accuracy = 100 * correct_ood / total_ood if total_ood > 0 else 0\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {total_loss / len(train_loader):.4f}, \"\n",
        "              f\"IND Accuracy: {ind_accuracy:.2f}%, OOD Accuracy: {ood_accuracy:.2f}%\")\n",
        "\n",
        "# Initialize model, optimizer, and device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ROWModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Run training for task 0\n",
        "train_feature_and_ood_head(\n",
        "    model,\n",
        "    task_id=0,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=train_loaders[0],\n",
        "    replay_buffer=replay_buffer,\n",
        "    epochs=10\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzL6rg4Lr1-e",
        "outputId": "9bc03cc9-01ac-4abf-ca07-62b909ab917c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 1.8123, IND Accuracy: 38.10%, OOD Accuracy: 0.00%\n",
            "Epoch [2/10] - Loss: 1.4943, IND Accuracy: 49.54%, OOD Accuracy: 0.00%\n",
            "Epoch [3/10] - Loss: 1.3180, IND Accuracy: 55.34%, OOD Accuracy: 0.00%\n",
            "Epoch [4/10] - Loss: 1.2134, IND Accuracy: 60.48%, OOD Accuracy: 0.00%\n",
            "Epoch [5/10] - Loss: 1.1041, IND Accuracy: 64.08%, OOD Accuracy: 0.00%\n",
            "Epoch [6/10] - Loss: 0.9851, IND Accuracy: 67.90%, OOD Accuracy: 0.00%\n",
            "Epoch [7/10] - Loss: 0.8587, IND Accuracy: 72.80%, OOD Accuracy: 0.00%\n",
            "Epoch [8/10] - Loss: 0.7603, IND Accuracy: 75.48%, OOD Accuracy: 0.00%\n",
            "Epoch [9/10] - Loss: 0.6748, IND Accuracy: 78.42%, OOD Accuracy: 0.00%\n",
            "Epoch [10/10] - Loss: 0.6448, IND Accuracy: 80.14%, OOD Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_wp_head(model, task_id, optimizer, train_loader, epochs=10):\n",
        "    \"\"\"\n",
        "    Fine-tunes the WP head for a specific task using only in-distribution data.\n",
        "\n",
        "    Args:\n",
        "    - model: ROWModel instance.\n",
        "    - task_id: ID of the current task.\n",
        "    - optimizer: Optimizer for training the WP head.\n",
        "    - train_loader: DataLoader for current task's IND data.\n",
        "    - epochs: Number of epochs for fine-tuning.\n",
        "    \"\"\"\n",
        "    # Freeze the feature extractor to prevent updates during WP head fine-tuning\n",
        "    for param in model.feature_extractor.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for ind_data, ind_labels in train_loader:\n",
        "            ind_data, ind_labels = ind_data.to(device), ind_labels.to(device)\n",
        "\n",
        "            # Forward pass through the WP head\n",
        "            optimizer.zero_grad()\n",
        "            wp_outputs = model(ind_data, task_id, head_type='wp')\n",
        "\n",
        "            # Compute cross-entropy loss for WP head on IND data\n",
        "            loss = F.cross_entropy(wp_outputs, ind_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(wp_outputs, 1)\n",
        "            correct += (predicted == ind_labels).sum().item()\n",
        "            total += ind_labels.size(0)\n",
        "\n",
        "        accuracy = 100 * correct / total if total > 0 else 0\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - WP Head Loss: {total_loss / len(train_loader):.4f}, \"\n",
        "              f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Unfreeze the feature extractor after fine-tuning if needed for further training\n",
        "    for param in model.feature_extractor.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Example usage for fine-tuning WP head of task 0\n",
        "optimizer = optim.Adam(model.wp_heads[0].parameters(), lr=0.001)  # Optimizer for only WP head parameters\n",
        "\n",
        "# Fine-tune WP head for task 0\n",
        "fine_tune_wp_head(\n",
        "    model,\n",
        "    task_id=0,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=train_loaders[0],\n",
        "    epochs=10\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0PH9_PAtsVu",
        "outputId": "c7a6dc4c-0227-4ad7-ab2e-136b391f0ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - WP Head Loss: 0.5163, Accuracy: 85.28%\n",
            "Epoch [2/10] - WP Head Loss: 0.2826, Accuracy: 91.34%\n",
            "Epoch [3/10] - WP Head Loss: 0.2484, Accuracy: 92.18%\n",
            "Epoch [4/10] - WP Head Loss: 0.2296, Accuracy: 92.84%\n",
            "Epoch [5/10] - WP Head Loss: 0.2131, Accuracy: 93.50%\n",
            "Epoch [6/10] - WP Head Loss: 0.2014, Accuracy: 93.78%\n",
            "Epoch [7/10] - WP Head Loss: 0.1945, Accuracy: 94.26%\n",
            "Epoch [8/10] - WP Head Loss: 0.1861, Accuracy: 94.38%\n",
            "Epoch [9/10] - WP Head Loss: 0.1789, Accuracy: 94.78%\n",
            "Epoch [10/10] - WP Head Loss: 0.1703, Accuracy: 95.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "def fine_tune_previous_ood_heads(model, current_task_id, optimizer, replay_buffer, epochs=3, batch_size=32, device='cpu'):\n",
        "    \"\"\"\n",
        "    Fine-tunes the OOD heads for previous tasks using replay data.\n",
        "\n",
        "    Args:\n",
        "    - model: ROWModel instance.\n",
        "    - current_task_id: The task ID that was most recently trained (all prior tasks will be fine-tuned).\n",
        "    - optimizer: Optimizer for training.\n",
        "    - replay_buffer: ReplayBuffer instance holding replay data.\n",
        "    - epochs: Number of epochs to fine-tune.\n",
        "    - batch_size: Size of the batch to sample from replay buffer.\n",
        "    - device: The device to perform computations on (e.g., 'cpu' or 'cuda').\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    # Loop over previous tasks\n",
        "    for task_id in range(current_task_id):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "\n",
        "            # Sample a batch from the replay buffer\n",
        "            batch = replay_buffer.sample(batch_size=batch_size)\n",
        "\n",
        "            # Debugging output\n",
        "            print(f\"Task {task_id}, Epoch {epoch + 1}: Raw batch output:\", batch)\n",
        "\n",
        "            # Check if the batch is valid\n",
        "            if batch is None or len(batch) < 2 or batch[0] is None or batch[1] is None:\n",
        "                print(f\"No valid replay data available for task {task_id}, skipping fine-tuning.\")\n",
        "                continue\n",
        "\n",
        "            # Unpack the batch into data and labels\n",
        "            ood_data, ood_labels = batch\n",
        "\n",
        "            # Ensure ood_data and ood_labels are PyTorch tensors\n",
        "            try:\n",
        "                if not isinstance(ood_data, torch.Tensor):\n",
        "                    ood_data = torch.tensor(ood_data, dtype=torch.float32)\n",
        "                if not isinstance(ood_labels, torch.Tensor):\n",
        "                    ood_labels = torch.tensor(ood_labels, dtype=torch.long)\n",
        "\n",
        "                # Debugging types and shapes\n",
        "                print(\"Type of ood_data:\", type(ood_data), \"Shape:\", ood_data.shape)\n",
        "                print(\"Type of ood_labels:\", type(ood_labels), \"Shape:\", ood_labels.shape)\n",
        "\n",
        "                # Move data to the appropriate device\n",
        "                ood_data, ood_labels = ood_data.to(device), ood_labels.to(device)\n",
        "\n",
        "                # Set OOD labels to a unique OOD class index for this task\n",
        "                ood_labels.fill_(model.ood_heads[task_id].fc.out_features - 1)\n",
        "\n",
        "                # Reset gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass through the model\n",
        "                ood_outputs = model(ood_data, task_id, head_type='ood')\n",
        "\n",
        "                # Compute loss for OOD head fine-tuning\n",
        "                loss = F.cross_entropy(ood_outputs, ood_labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                print(f\"Task {task_id}, Epoch [{epoch + 1}/{epochs}], Fine-Tuning OOD Loss: {total_loss:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing batch for task {task_id}, Epoch {epoch + 1}: {e}\")\n",
        "\n",
        "# Example usage for fine-tuning previous OOD heads after training a new task\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "fine_tune_previous_ood_heads(\n",
        "    model,\n",
        "    current_task_id=1,\n",
        "    optimizer=optimizer,\n",
        "    replay_buffer=replay_buffer,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAlimwAVOYxX",
        "outputId": "84de7352-0f80-409b-bee5-dd230ec0b1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 0, Epoch 1: Raw batch output: (None, None)\n",
            "No valid replay data available for task 0, skipping fine-tuning.\n",
            "Task 0, Epoch 2: Raw batch output: (None, None)\n",
            "No valid replay data available for task 0, skipping fine-tuning.\n",
            "Task 0, Epoch 3: Raw batch output: (None, None)\n",
            "No valid replay data available for task 0, skipping fine-tuning.\n",
            "Task 0, Epoch 4: Raw batch output: (None, None)\n",
            "No valid replay data available for task 0, skipping fine-tuning.\n",
            "Task 0, Epoch 5: Raw batch output: (None, None)\n",
            "No valid replay data available for task 0, skipping fine-tuning.\n",
            "Task 0, Epoch 6: Raw batch output: (None, None)\n",
            "No valid replay data available for task 0, skipping fine-tuning.\n",
            "Task 0, Epoch 7: Raw batch output: (None, None)\n",
            "No valid replay data available for task 0, skipping fine-tuning.\n",
            "Task 0, Epoch 8: Raw batch output: (None, None)\n",
            "No valid replay data available for task 0, skipping fine-tuning.\n",
            "Task 0, Epoch 9: Raw batch output: (None, None)\n",
            "No valid replay data available for task 0, skipping fine-tuning.\n",
            "Task 0, Epoch 10: Raw batch output: (None, None)\n",
            "No valid replay data available for task 0, skipping fine-tuning.\n"
          ]
        }
      ]
    }
  ]
}